## ğŸš€ Quick Start: Production Ingest

### Run Ingest with Progress Monitor

**Terminal 1: Start ingest**
```bash
cd c:\Users\naren\Sovereign
python cold_strategist/scripts/ingest_books.py books --mode full
```

**Terminal 2: Monitor progress**
```bash
cd c:\Users\naren\Sovereign
python utils/monitor_progress.py
```

You'll see:
```
==================================================
ğŸ“ˆ INGEST PROGRESS MONITOR
==================================================
Progress    :   25.00%
Completed   :   100 / 2045
Rate        : 5.4 chunks/sec
ETA         : 6m 1s
==================================================
```

### Key Features (All Working âœ…)

| Feature | How It Works |
|---------|-------------|
| **Resume-after-crash** | Progress file survives kills. Restart = continue. |
| **Deduplication** | Chunk hashes prevent re-embedding. Auto-skip. |
| **Parallel embedding** | 2 concurrent threads (RTX 4060 safe) |
| **GPU protection** | Semaphore prevents VRAM overload |
| **Live ETA** | Calculated from actual throughput (updated every 2s) |
| **Crash-safe logging** | Append-only ledger survives power loss |
| **Deterministic** | Same input always = same result |

### Files You Need to Know

```
utils/
  â”œâ”€â”€ hash.py                 â† Chunk fingerprinting
  â”œâ”€â”€ progress.py             â† Crash-safe ledger
  â”œâ”€â”€ embedding_guard.py      â† GPU semaphore
  â”œâ”€â”€ eta.py                  â† ETA computation
  â”œâ”€â”€ metrics.py              â† Progress file handler
  â”œâ”€â”€ monitor_progress.py     â† Live monitor
  â””â”€â”€ monitor_progress.ps1    â† PowerShell monitor

cold_strategist/
  â”œâ”€â”€ core/knowledge/ingest/indexer.py     â† Parallel embed + dedup
  â””â”€â”€ state/ingest_progress.jsonl          â† Crash-safe ledger
  
core/knowledge/ingest/indexer.py            â† Parallel embed + dedup
cold_strategist/state/ingest_metrics.json   â† Live progress
```

### One-Liner Check (Any Time)

```bash
python -c "import json; from pathlib import Path; from utils.eta import format_eta; d=json.loads(Path('cold_strategist/state/ingest_metrics.json').read_text()); print(f'{d[\"percent_complete\"]}% | {d[\"completed_chunks\"]}/{d[\"total_chunks\"]} | ETA: {format_eta(d.get(\"eta_seconds\"))}')"
```

### Test Resume (Crash Recovery)

1. **Start ingest:**
   ```bash
   python cold_strategist/scripts/ingest_books.py books --mode full
   ```

2. **Wait ~30 seconds, then Ctrl+C** (kill it mid-process)

3. **Restart immediately:**
   ```bash
   python cold_strategist/scripts/ingest_books.py books --mode full
   ```

4. **Check progress:**
   ```bash
   python utils/monitor_progress.py
   ```

You'll see it continues from where it left off (same chunk counts resume).

### Throughput Expected

- **Embedding:** ~1.7-5.7 chunks/sec (depends on Ollama load)
- **ETA:** Auto-calculated (e.g., 100 chunks @ 5/sec = 20 sec remaining)
- **Total ingest:** ~2045 chunks â‰ˆ 6-30 minutes (varies by model)

### Troubleshooting

| Issue | Fix |
|-------|-----|
| Monitor shows "calculating..." | Ingest just started, wait 10 sec |
| ETA seems wrong | It's recalculating from actual rate, wait a bit |
| Metrics file missing | Run `python -c "from utils.metrics import init_metrics; init_metrics(2045)"` |
| GPU semaphore blocking | Expected. Max 2 concurrent embeds intentional. |
| Progress stuck | Check `cold_strategist/state/ingest_progress.jsonl` for errors |

### Canonical Architecture

```
Your Python Code (localhost)
    â†“
Ollama (127.0.0.1:11434)
    â†“
Models:
  - DeepSeek R1 (reasoning)
  - Qwen2.5 (coding)
  - Dolphin (adversarial)
  - Nomic-Embed (vectors)
```

No WebUI in path. No localhost hacks. Pure native Ollama API.

---

**You're all set.** Start ingest + monitor. It'll auto-resume if it crashes.
