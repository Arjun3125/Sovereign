chapter 4.
In March 1960, the U.S. Secret Service began to mobilise anti-communist

chapter 4.
In March 1960, the U.S. Secret Service began to mobilise anti-communist
exiles from Cuba, most of them living in Miami, to use against Fidel Castro’s
regime. In January 1961, two days after taking office, President Kennedy was
informed about the secret plan to invade Cuba. Three months later, a key meeting
took place at the White House in which Kennedy and his advisers all voted in
favour of the invasion. On 17 April 1961, a brigade of 1,400 exiled Cubans landed
at the Bay of Pigs, on Cuba’s south coast, with the help of the U.S. Navy, the Air
Force and the CIA. The aim was to overthrow Castro’s government. However,
nothing went as planned. On the first day, not a single supply ship reached the
coast. The Cuban air force sank the first two and the next two turned around and
fled back to the U.S. A day later, Castro’s army completely surrounded the
brigade. On the third day, the 1,200 survivors were taken into custody and sent to
military prisons. Kennedy’s invasion of the Bay of Pigs is regarded as one of the
biggest flops in American foreign policy. That such an absurd plan was ever
agreed upon, never mind put into action, is astounding. All of the assumptions
that spoke in favour of invasion were erroneous. For example, Kennedy’s team
completely underestimated the strength of Cuba’s air force. Also, it was expected
that, in an emergency, the brigade would be able to hide in the Escambray
mountains and carry out an underground war against Castro from there. A glance
at the map shows that the refuge was 100 miles away from the Bay of Pigs, with

[PAGE 66]
an insurmountable swamp in between. And yet, Kennedy and his advisers were
among the most intelligent people to ever run an American government. What
went wrong between January and April of 1961?
Psychology professor Irving Janis has studied many fiascos. He concluded that
they share the following pattern: members of a close-knit group cultivate team
spirit by (unconsciously) building illusions. One of these fantasies is a belief in
invincibility: ‘If both our leader [in this case, Kennedy] and the group are confident
that the plan will work, then luck will be on our side.’ Next comes the illusion of
unanimity: if the others are of the same opinion, any dissenting view must be
wrong. No one wants to be the naysayer who destroys team unity. Finally, each
person is happy to be part of the group. Expressing reservations could mean
exclusion from it. In our evolutionary past, such banishment guaranteed death;
hence our strong urge to remain in the group’s favour.
The business world is no stranger to groupthink. A classic example is the fate
of the world-class airline Swissair. Here, a group of highly paid consultants rallied
around the former CEO and, bolstered by the euphoria of past successes,
developed a high-risk expansion strategy (including the acquisition of several
European airlines). The zealous team built up such a strong consensus that even
rational reservations were suppressed, leading to the airline’s collapse in 2001.
If you ever find yourself in a tight, unanimous group, you must speak your mind,
even if your team does not like it. Question tacit assumptions, even if you risk
expulsion from the warm nest. And, if you lead a group, appoint someone as
devil’s advocate. She will not be the most popular member of the team, but she
might be the most important.
See also Social Proof (ch. 4); Social Loafing (ch. 33); In-Group Out-Group Bias (ch. 79);
Planning Fallacy (ch. 91)

[PAGE 67]
26
WHY YOU’LL SOON BE PLAYING MEGATRILLIONS
Neglect of Probability
Two games of chance: in the first, you can win $10 million, and in the second,
$10,000. Which do you play? If you win the first game, it changes your life
completely: you can quit your job, tell your boss where to go and live off the
winnings. If you hit the jackpot in the second game, you can take a nice vacation
in the Caribbean, but you’ll be back at your desk quick enough to see your
postcard arrive. The probability of winning is one in 100 million in the first game,
and one in 10,000 in the second game. So which do you choose?
Our emotions draw us to the first game, even though the second is ten times
better, objectively considered (expected win times probability). Therefore, the
trend is towards ever-larger jackpots – Mega Millions, Mega Billions, Mega
Trillions – no matter how small the odds are.
In a classic experiment from 1972, participants were divided into two groups.
The members of the first group were told that they would receive a small electric
shock. In the second group, subjects were told that the risk of this happening was
only 50%. The researchers measured physical anxiety (heart rate, nervousness,
sweating, etc.) shortly before commencing. The result were, well, shocking: there
was absolutely no difference. Participants in both groups were equally stressed.
Next, the researchers announced a series of reductions in the probability of a
shock for the second group: from 50% to 20%, then 10%, then 5%. The result: still
no difference! However, when they declared they would increase the strength of
the expected current, both groups’ anxiety levels rose – again, by the same
degree. This illustrates that we respond to the expected magnitude of an event
(the size of the jackpot or the amount of electricity), but not to its likelihood. In
other words: we lack an intuitive grasp of probability.
The proper term for this is neglect of probability, and it leads to errors in
decision-making. We invest in start-ups because the potential profit makes dollar
signs flash before our eyes, but we forget (or are too lazy) to investigate the slim
chances of new businesses actually achieving such growth. Similarly, following
extensive media coverage of a plane crash, we cancel flights without really

[PAGE 68]
considering the minuscule probability of crashing (which, of course, remains the
same before and after such a disaster). Many amateur investors compare their
investments solely on the basis of yield. For them, Google shares with a return of
20% must be twice as good as property that returns 10%. That’s wrong. It would
be a lot smarter to also consider both investments’ risks. But then again, we have
no natural feel for this so we often turn a blind eye to it.
Back to the experiment with the electric shocks: in group B, the probability of
getting a jolt was further reduced: from 5% to 4% to 3%. Only when the probability
reached zero did group B respond differently to group A. To us, 0% risk seems
infinitely better than a (highly improbable) 1% risk.
To test this, let’s examine two methods of treating drinking water. Suppose a
river has two equally large tributaries. One is treated using method A, which
reduces the risk of dying from contaminated water from 5% to 2%. The other is
treated using method B, which reduces the risk from 1% to 0%, i.e. the threat is
completely eliminated. So, method A or B? If you think like most people, you will
opt for method B – which is silly because with measure A, 3% fewer people die,
and with B, just 1% fewer. Method A is three times as good! This fallacy is called
the zero-risk bias.
A classic example of this is the U.S. Food Act of 1958, which prohibits food that
contains cancer-causing substances. Instituted to achieve zero risk of cancer, this
ban sounds good at first, but it ended up leading to the use of more dangerous
(but non-carcinogenic) food additives. It is also absurd: as Paracelsus illustrated
in the sixteenth century, poisoning is always a question of dosage. Furthermore,
this law can never be enforced properly since it is impossible to remove the last
‘banned’ molecule from food. Each farm would have to function like a hyper-
sterile computer-chip factory, and the cost of food would increase a hundredfold.
Economically, zero risk rarely makes sense. One exception is when the
consequences are colossal, such as a deadly, highly contagious virus escaping
from a biotech laboratory.
We have no intuitive grasp of risk and thus distinguish poorly between different
threats. The more serious the threat and the more emotional the topic (such as
radioactivity), the less reassuring a reduction in risk seems to us. Two
researchers at the University of Chicago have shown that people are equally

[PAGE 69]
afraid of a 99% chance as they are of a 1% chance of contamination by toxic
chemicals. An irrational response, but a common one.
See also Availability Bias (ch. 11); Base-Rate Neglect (ch. 28); Problem with Averages
(ch. 55); Survivorship Bias (ch. 1); Illusion of Control (ch. 17); Exponential Growth (ch.
34); Ambiguity Aversion (ch. 80)

[PAGE 70]
27
WHY THE LAST COOKIE IN THE JAR MAKES YOUR MOUTH
WATER
Scarcity Error
Coffee at a friend’s house. We sat trying to make conversation while her three
children grappled with one another on the floor. Suddenly I remembered that I
had brought some glass marbles with me – a whole bag full. I spilled them out on
the floor, in the hope that the little angels would play with them in peace. Far from
it: a heated argument ensued. I didn’t understand what was happening until I
looked more closely. Among the countless marbles there was just one blue one,
and the children scrambled for it. All the marbles were exactly the same size and
shiny and bright. But the blue one had an advantage over the others – it was one
of a kind. I had to laugh at how childish children are!
In August 2005, when I heard that Google would launch its own email service, I
was dead set on getting an account. (In the end I did.) At the time, new accounts
were very restricted and were given out only on invitation. This made me want
one even more. But why? Certainly not because I needed another email account
(back then, I already had four), nor because Gmail was better than the
competition, but simply because not everyone had access to it. Looking back, I
have to laugh at how childish adults are!
Rara sunt cara, said the Romans. Rare is valuable. In fact, the scarcity error is
as old as mankind. My friend with the three children is a part-time real-estate
agent. Whenever she has an interested buyer who cannot decide, she calls and
says ‘A doctor from London saw the plot of land yesterday. He liked it a lot. What
about you? Are you still interested?’ The doctor from London – sometimes it’s a
professor or a banker – is, of course, fictitious. The effect is very real, though: it
causes prospects to see the opportunity disappearing before their eyes, so they
act and close the deal. Why? This is the potential shortage of supply, yet again.
Objectively, this situation is incomprehensible: either the prospect wants the land
for the set price or he does not – regardless of any doctors from London.
To assess the quality of cookies, Professor Stephen Worchel split participants
into two groups. The first group received an entire box of cookies, and the second

[PAGE 71]
group just two. In the end, the subjects with just two cookies rated the quality
much higher than the first group did. The experiment was repeated several times
and always showed the same result.
‘Only while stocks last,’ the adverts alert. ‘Today only,’ warn the posters.
Gallery owners take advantage of the scarcity error by placing red ‘sold’ dots
under most of their paintings, transforming the remaining few works into rare
items that must be snatched up quickly. We collect stamps, coins, vintage cars
even when they serve no practical purpose. The post office doesn’t accept the old
stamps, the banks don’t take old coins, and the vintage cars are no longer
allowed on the road. These are all side issues; the attraction is that they are in
short supply.
In one study, students were asked to arrange ten posters in order of
attractiveness – with the agreement that afterward they could keep one poster as
a reward for their participation. Five minutes later, they were told that the poster
with the third highest rating was no longer available. Then they were asked to
judge all ten from scratch. The poster that was no longer available was suddenly
classified as the most beautiful. In psychology, this phenomenon is called
reactance: when we are deprived of an option, we suddenly deem it more
attractive. It is a kind of act of defiance. It is also known as the Romeo and Juliet
effect: because the love between the tragic Shakespearean teenagers is
forbidden, it knows no bounds. This yearning does not necessarily have to be a
romantic one; in the U.S., student parties are often littered with desperately drunk
teenagers – just because it’s illegal to drink below the age of 21.
In conclusion: the typical response to scarcity is a lapse in clear thinking.
Assess products and services solely on the basis of their price and benefits. It
should be of no importance if an item is disappearing fast, nor if any doctors from
London take an interest.
See also Contrast Effect (ch. 10); Fear of Regret (ch. 82); House-Money Effect (ch. 84)

[PAGE 72]
28
WHEN YOU HEAR HOOFBEATS, DON’T EXPECT A ZEBRA
Base-Rate Neglect
Mark is a thin man from Germany with glasses who likes to listen to Mozart.
Which is more likely? That Mark is A) a truck driver or B) a professor of literature
in Frankfurt. Most will bet on B, which is wrong. Germany has 10,000 times more
truck drivers than Frankfurt has literature professors. Therefore, it is more likely
that Mark is a truck driver. So what just happened? The detailed description
enticed us to overlook the statistical reality. Scientists call this fallacy base-rate
neglect: a disregard of fundamental distribution levels. It is one of the most
common errors in reasoning. Virtually all journalists, economists and politicians
fall for it on a regular basis.
Here is a second example: a young man is stabbed and fatally injured. Which
of these is more likely? A) The attacker is a Russian immigrant and imports
combat knives illegally, or B) the attacker is a middle-class American. You know
the drill now: option B is much more likely because there are a million times more
middle-class Americans than there are Russian knife importers.
In medicine, base-rate neglect plays an important role. For example, migraines
can point (among others) to a viral infection or a brain tumour. However, viral
infections are much more common (in other words, they have a higher base rate),
so doctors assess patients for these first before testing for tumours. This is very
reasonable. In medical school, residents spend a lot of time purging base-rate
neglect. The motto drummed into any prospective doctor in the United States is:
‘When you hear hoofbeats behind you, don’t expect to see a zebra.’ Which
means: investigate the most likely ailments before you start diagnosing exotic
diseases, even if you are a specialist in that.
Doctors are the only professionals who enjoy this base-rate training.
Regrettably, few people in business are exposed to it. Now and then I see high-
flying entrepreneurs’ business plans and get very excited by their products, ideas
and personalities. I often catch myself thinking: this could be the next Google! But
a glance at the base rate brings me back down to earth. The probability that a firm
will survive the first five years is 20%. So what then is the probability that they will

[PAGE 73]
grow into a global corporation? Almost zero. Warren Buffett once explained why
he does not invest in biotech companies: ‘How many of these companies make a
turnover of several hundred million dollars? It simply does not happen?. . .?The
most likely scenario is that these firms will just hover somewhere in the middle.’
This is clear base-rate thinking. For most people, survivorship bias (chapter 1) is
one of the causes for their base-rate neglect. They tend to see only the successful
individuals and companies, because the unsuccessful cases are not reported (or
are under-reported). This makes them neglect the large part of the ‘invisible’
cases.
Imagine you are sampling wine in a restaurant and have to guess from which
country it comes. The label of the bottle is covered. If, like me, you are not a wine
connoisseur, the only lifeline you have is the base rate. You know from
experience that about three-quarters of the wines on the menu are of French
origin, so reasonably, you guess France, even if you suspect a Chilean or
Californian twist.
Sometimes I have the dubious honour of speaking in front of students of elite
business schools. When I ask them about their career prospects, most answer
that in the medium term, they see themselves on the boards of global companies.
Years ago, both my fellow students and I gave the same answer. The way I see it,
my role is to give students a base-rate crash course: ‘With a degree from this
school, the chance of you landing a spot on the board of a Fortune 500 company
is less than 0.1%. No matter how smart and ambitious you are, the most likely
scenario is that you will end up in middle management.’ With this, I earn shocked
looks, and tell myself that I have made a small contribution toward mitigating their
future mid-life crises.
See also Survivorship Bias (ch. 1); Neglect of Probability (ch. 26); Gambler’s Fallacy
(ch. 29); Conjunction Fallacy (ch. 41); The Problem with Averages (ch. 55); Information
Bias (ch. 59); Ambiguity Aversion (ch. 80)

[PAGE 74]
29
WHY THE ‘BALANCING FORCE OF THE UNIVERSE’ IS
BALONEY
Gambler’s Fallacy
In the summer of 1913, something incredible happened in Monte Carlo. Crowds
gathered around a roulette table and could not believe their eyes. The ball had
landed on black twenty times in a row. Many players took advantage of the
opportunity and immediately put their money on red. But the ball continued to
come to rest on black. Even more people flocked to the table to bet on red. It had
to change eventually! But it was black yet again – and again and again. It was not
until the twenty-seventh spin that the ball eventually landed on red. By that time,
the players had bet millions on the table. In a few spins of the wheel, they were
bankrupt.
The average IQ of pupils in a big city is 100. To investigate this, you take a
random sample of 50 students. The first child tested has an IQ of 150. What will
the average IQ of your 50 students be? Most people guess 100. Somehow, they
think that the super-smart student will be balanced out – perhaps by a dismal
student with an IQ of 50 or by two below-average students with IQs of 75. But with
such a small sample, that is very unlikely. We must expect that the remaining 49
students will represent the average of the population, so they will each have an
average IQ of 100. Forty-nine times 100 plus one IQ of 150 gives us an average
of 101 in the sample.
The Monte Carlo example and the IQ experiment show that people believe in
the ‘balancing force of the universe’. This is the gambler’s fallacy. However, with
independent events, there is no harmonising force at work: a ball cannot
remember how many times it has landed on black. Despite this, one of my friends
enters the weekly Mega Millions numbers into a spreadsheet, and then plays
those that have appeared the least. All this work is for naught. He is another
victim of the gambler’s fallacy.
The following joke illustrates this phenomenon: a mathematician is afraid of
flying due to the small risk of a terrorist attack. So, on every flight he takes a bomb
with him in his hand luggage. ‘The probability of having a bomb on the plane is

[PAGE 75]
very low,’ he reasons, ‘and the probability of having two bombs on the same
plane is virtually zero!’
A coin is flipped three times and lands on heads on each occasion. Suppose
someone forces you to spend thousands of dollars of your own money betting on
the next toss. Would you bet on heads or tails? If you think like most people, you
will choose tails, although heads is just as likely. The gambler’s fallacy leads us
to believe that something must change.
A coin is tossed 50 times, and each time it lands on heads. Again, with
someone forcing you to bet, do you pick heads or tails? Now that you’ve seen an
example or two, you’re wise to the game: you know that it could go either way.
But we’ve just come across another pitfall: the classic déformation
professionnelle (professional oversight) of mathematicians: common sense would
tell you that heads is the wiser choice, since the coin is obviously loaded.
Previously, we looked at regression to mean. An example: if you are
experiencing record cold where you live, it is likely that the temperature will return
to normal values over the next few days. If the weather functioned like a casino,
there would be a 50% chance that the temperature would rise and a 50% chance
that it would drop. But the weather is not like a casino. Complex feedback
mechanisms in the atmosphere ensure that extremes balance themselves out. In
other cases, however, extremes intensify. For example, the rich tend to get richer.
A stock that shoots up creates its own demand to a certain extent, simply because
it stands out so much – a sort of reverse compensation effect.
So, take a closer look at the independent and interdependent events around
you. Purely independent events really only exist at the casino, in the lottery and in
theory. In real life, in the financial markets and in business, with the weather and
your health, events are often interrelated. What has already happened has an
influence on what will happen. As comforting an idea as it is, there is simply no
balancing force out there for independent events. ‘What goes around, comes
around’ simply does not exist.
See also Problem with Averages (ch. 55); Base-Rate Neglect (ch. 28); Déformation
Professionnelle (ch. 92); Regression to the Mean (ch. 19); Simple Logic (ch. 63)

[PAGE 76]
30
WHY THE WHEEL OF FORTUNE MAKES OUR HEADS SPIN
The Anchor
When was Abraham Lincoln born? If you don’t know the year off the top of your
head, and your smartphone battery has just died, how do you answer this?
Perhaps you know that he was president during the Civil War in the 1860s and
that he was the first U.S. president to be assassinated. Looking at the Lincoln
Memorial in Washington, you don’t see a young, energetic man but something
more akin to a worn-out 60-year-old veteran. The memorial must depict him at the
height of his political power, say at the age of 60. Let’s assume that he was
assassinated in the mid-1860s, making 1805 our estimate for the year he was
born. (The correct answer is 1809.) So how did we work it out? We found an
anchor to help us – the 1860s – and worked from there to an educated guess.
Whenever we have to guess something – the length of the Mississippi River,
population density in Russia, the number of nuclear power plants in France – we
use anchors. We start with something we are sure of and venture into unfamiliar
territory from there. How else could we do it? Just pick a number off the top of our
heads? That would be irrational.
Unfortunately, we also use anchors when we don’t need to. For example, one
day in a lecture, a professor placed a bottle of wine on the table. He asked his
students to write down the last two digits of their social security numbers and then
decide if they would be willing to spend that amount on the wine. In the auction
that followed, students with higher numbers bid nearly twice as much as students
with lower numbers. The social security digits worked as an anchor – albeit in a
hidden and misleading way.
The psychologist Amos Tversky conducted an experiment involving a wheel of
fortune. He had participants spin it, and afterward, they were asked how many
member states the United Nations has. Their guesses confirmed the anchor
effect: the highest estimates came from people who had spun high numbers on
the wheel.
Researchers Russo and Shoemaker asked students in what year Attila the Hun
suffered his crushing defeat in Europe. Just like the example with social security

[PAGE 77]
numbers, the participants were anchored – this time with the last few digits of their
telephone number. The result? People with higher numbers chose later years
and vice versa. (If you were wondering, Attila’s demise came about in 453.)
Another experiment: students and professional real-estate agents were given a
tour of a house and asked to estimate its value. Beforehand, they were informed
about a (randomly generated) listed sales price. As might be expected, the
anchor influenced the students: the higher this price, the higher they valued the
property. And the professionals? Did they value the house objectively? No, they
were similarly influenced by the random anchor amount. The more uncertain the
value of something – such as real estate, company stock or art – the more
susceptible even experts are to anchors.
Anchors abound, and we all clutch at them. The ‘recommended retail price’
printed on many products is nothing more than an anchor. Sales professionals
know they must establish a price at an early stage – long before they have an
offer. Also, it has been proven that if teachers know students’ past grades, it
influences how they will mark new work. The most recent grades act as a starting
point.
In my early years, I had a quick stint at a consulting firm. My boss was a pro
when it came to using anchors. In his first conversation with any client, he made
sure to fix an opening price, which, by the way, almost criminally exceeded our
internal costs: ‘I’ll tell you this now so you’re not surprised when you receive the
quote, Mr. So-and-So: we’ve just completed a similar project for one of your
competitors and it was in the range of five million dollars.’ The anchor was
dropped: the price negotiations started at exactly five million.
See also Framing (ch. 42)

[PAGE 78]
31
HOW TO RELIEVE PEOPLE OF THEIR MILLIONS
Induction
A farmer feeds a goose. At first, the shy animal is hesitant, wondering ‘What’s
going on here? Why is he feeding me?’ This continues for a few more weeks
until, eventually, the goose’s scepticism gives way. After a few months, the goose
is sure that ‘The farmer has my best interests at heart.’ Each additional day’s
feeding confirms this. Fully convinced of the man’s benevolence, the goose is
amazed when he takes it out of its enclosure on Christmas Day – and slaughters
it. The Christmas goose fell victim to inductive thinking, the inclination to draw
universal certainties from individual observations. Philosopher David Hume used
this allegory back in the eighteenth century to warn of its pitfalls. However, it’s not
just geese that are susceptible to it.
An investor buys shares in stock X. The share price rockets, and at first he is
wary. ‘Probably a bubble,’ he suspects. As the stock continues to rise, even after
months, his apprehension turns into excitement: ‘This stock may never come
down’ – especially since every day this is the case. After half a year, he invests
his life savings in it, turning a blind eye to the huge cluster risk this poses. Later,
the man will pay for his foolish investment. He has fallen hook, line and sinker for
induction.
Inductive thinking doesn’t have to be a road to ruin, though. In fact, you can
make a fortune with it by sending a few emails. Here’s how: put together two
stock market forecasts – one predicting that prices will rise next month and one
warning of a drop. Send the first email to 50,000 people, and the second email to
a different set of 50,000. Suppose that after one month, the indices have fallen.
Now you can send another email, but this time only to the 50,000 people who
received a correct prediction. These 50,000 you divide into two groups: the first
half learns that prices will increase next month, the second half discovers they
will fall. Continue doing this. After 10 months, around 100 people will remain, all
of whom you have advised impeccably. From their perspective, you are a genius.
You have proven that you are truly in possession of prophetic powers. Some of
these people will trust you with their money. Take it and start a new life in Brazil.

[PAGE 79]
However, it’s not just naïve strangers who get deceived in this way; we
constantly trick ourselves, too. For example, people who are rarely ill consider
themselves immortal. CEOs who announce increased profits in consecutive
quarters deem themselves infallible – their employees and shareholders do, too. I
once had a friend who was a base jumper. He jumped off cliffs, antennae, and
buildings, pulling the ripcord only at the last minute. One day, I brought up how
risky his chosen sport is. He replied quite matter-of-factly: ‘I’ve over 1,000 jumps
under my belt, and nothing has ever happened to me.’ Two months later, he was
dead. It happened when he jumped from a particularly dangerous cliff in South
Africa. This single event was enough to eradicate a theory confirmed a thousand
times over.
Inductive thinking can have devastating results. Yet we cannot do without it.
We trust that, when we board a plane, aerodynamic laws will still be valid. We
imagine that we will not be randomly beaten up on the street. We expect that our
hearts will still be beating tomorrow. These are confidences without which we
could not live, but we must remember that certainties are always provisional. As
Benjamin Franklin said, ‘Nothing is certain but death and taxes.’
Induction seduces us and leads us to conclusions such as: ‘Mankind has
always survived, so we will be able to tackle any future challenges, too.’ Sounds
good in theory, but what we fail to realise is that such a statement can only come
from a species that has lasted until now. To assume that our existence to date is
an indication of our future survival is a serious flaw in reasoning. Probably the
most serious of all.
See also False Causality (ch.37); Survivorship Bias (ch. 1)

[PAGE 80]
32
WHY EVIL STRIKES HARDER THAN GOOD
Loss Aversion
On a scale of 1 to 10, how good do you feel today? Now consider what would
bring you up to a perfect 10. That vacation in the Caribbean you’ve always
dreamed of? A step up the career ladder maybe? Next question: what would
make you drop down by the same number of points? Paralysis, Alzheimer’s,
cancer, depression, war, hunger, torture, financial ruin, damage to your
reputation, losing your best friend, your children getting kidnapped, blindness,
death? The long list of possibilities makes us realise just how many obstacles to
happiness exist; in short, there are more bad things than good – and they are far
more consequential.
In our evolutionary past, this was even more the case. One stupid mistake and
you were dead. Everything could lead to your rapid departure from the game of
life – carelessness on the hunt, an inflamed tendon, exclusion from the group and
so on. People who were reckless or gung-ho died before they could pass their
genes on to the next generation. Those who remained, the cautious, survived. We
are their descendants.
So, no wonder we fear loss more than we value gain. Losing $100 costs you a
greater amount of happiness than the delight you would feel if I gave you $100. In
fact, it has been proven that, emotionally, a loss ‘weighs’ about twice that of a
similar gain. Social scientists call this loss aversion.
For this reason, if you want to convince someone about something, don’t focus
on the advantages; instead highlight how it helps them dodge the disadvantages.
Here is an example from a campaign promoting breast self-examination (BSE):
two different leaflets were handed out to women. Pamphlet A urged: ‘Research
shows that women who do BSE have an increased chance of finding a tumour in
the early, more treatable state of the disease.’ Pamphlet B said: ‘Research shows
that women who do not do BSE have a decreased chance of finding a tumour in
the early, more treatable state of the disease.’ The study revealed that pamphlet B
(written in a ‘loss-frame’) generated significantly more awareness and BSE
behaviour than pamphlet A (written in a ‘gain-frame’).

[PAGE 81]
The fear of losing something motivates people more than the prospect of
gaining something of equal value. Suppose your business is home insulation.
The most effective way of encouraging customers to purchase your product is to
tell them how much money they are losing without insulation – as opposed to
how much money they would save with it, even though the amount is exactly the
same.
This type of aversion is also found on the stock market, where investors tend to
simply ignore losses on paper. After all, an unrealised loss isn’t as painful as a
realised one. So they sit on the stock, even if the chance of recovery is small and
the probability of further decline is large. I once met a man, a multimillionaire, who
was terribly upset because he had lost a $100 bill. What a waste of emotion! I
pointed out that the value of his portfolio fluctuated by at least $100 every second.
Management gurus push employees in large companies to be bolder and more
entrepreneurial. The reality is: employees tend to be risk-averse. From their
perspective, this aversion makes perfect sense: why risk something that brings
them, at best, a nice bonus, and at worst, a pink slip? The downside is larger than
the upside. In almost all companies and situations, safeguarding your career
trumps any potential reward. So, if you’ve been scratching your head about the
lack of risk-taking among your employees, you now know why. (However, if
employees do take big risks, it is often when they can hide behind group
decisions. Learn more in